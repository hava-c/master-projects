{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onLsXmM1C3v7"
   },
   "source": [
    "# Assignment - Binary classification on the Leukemia dataset using Elastic Net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jcRBZlqEC3v-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGA65bHeC3v_"
   },
   "source": [
    "## Loading and pre-processing of the dataset\n",
    "\n",
    "In this project we will use the ```leukemia``` dataset from the ```sklearn``` library, a high-dimensional dataset of genetic data where $p >> n$. More precisely, the dataset consists of $72$ observations and $7129$ features. Our goal is to perform binary classification using these features to determine the type of leukemia of each patient. It is natural to chose regularized logistic regression models to handle this task. Let us first load and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpT3xuCHC3v_",
    "outputId": "328688a7-6fce-4e9f-9ef8-91cba61ae0cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Done !\n",
      "n = 72, p = 7129\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "dataset = fetch_openml(\"leukemia\")\n",
    "\n",
    "X = dataset.data.astype(float)\n",
    "y = (dataset.target!='ALL').astype(int)\n",
    "print('Done !')\n",
    "print('n = {}, p = {}'.format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H5jubEgE-Aq"
   },
   "source": [
    "In general when training a supervised model, the first thing to do is to split our dataset into train, validation and test subsets. However, the ```leukemia``` dataset has very small number of observations, and therefore we keep just a small subset for the test such as $30$%, and use the rest for training. We also scale the data before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLlxvg9MFkb9",
    "outputId": "215f5372-b6b0-4f10-b18f-eacf92c2c308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 50, Test: 22\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_ = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.3)\n",
    "print('Train: {}, Test: {}'.format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idrVGSl6FXc_"
   },
   "source": [
    "## Classification using Logistic regression\n",
    "\n",
    "Let us first try to classify the dataset using a classic logistic regression, that we will then use as a baseline. We will compare the performance as well as the usage of the features of the logistic regression with the results of regularized regressions. Let $y_i = \\{0,1\\}$ be the variable we want to predict for an observation $i$ (in our case the type of leukemia) and $x_i = (x_i^{(1)}, \\dots, x_i^{(p)})$ the associated features. The probability for an observation $i$ to belong to either class $0$ or class $1$ is given by\n",
    "\n",
    "$$p(y_i|x_i, \\beta) =  p_i^{y_i}(1-p_i)^{1- y_i}$$\n",
    "\n",
    "With $p_i = \\frac{1}{1+\\exp(-\\beta^Tx_i)}$. We are mostly interested in estimating $\\beta \\in \\mathbb{R}^p$ to be able to predict class of new data points. The traditional approach to logistic regression is through maximum likelihood. The log-likelihood of logistic regression is given by\n",
    "\n",
    "$$ \\ell(\\beta) = \\sum_{i=1}^n y_i \\log p_i + (1- y_i) \\log (1 - p_i)$$\n",
    "\n",
    "The maximum likelihood estimator $\\hat{\\beta}$, if it exists, is simply the solution to the problem\n",
    "\n",
    "$$ \\hat{\\beta} = \\underset{\\beta \\in \\mathbb{R}^p}{\\operatorname{argmax}} \\ell(\\beta) $$\n",
    "\n",
    "For the implementation we will use the ```LogisticRegression``` function from the ```sklearn``` python library, where we set ```penalty='none'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjhEgCMVFbSx",
    "outputId": "838c354b-a3bb-4645-b2cb-a0e46f9be271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 100.00%\n",
      "Accuracy on test set: 86.36%\n",
      "Number of positive coefficients:  3666\n",
      "Number of negative coefficients:  3463\n",
      "Number of null coefficients:  0\n"
     ]
    }
   ],
   "source": [
    "clf_logistic = LogisticRegression(penalty = 'none', solver = 'saga', max_iter=10000)\n",
    "clf_logistic.fit(X_train, y_train)\n",
    "print('Accuracy on train set: {:.2f}%'.format(clf_logistic.score(X_train, y_train)*100))\n",
    "print('Accuracy on test set: {:.2f}%'.format(clf_logistic.score(X_test, y_test)*100))\n",
    "print('Number of positive coefficients: ', np.sum(clf_logistic.coef_>0))\n",
    "print('Number of negative coefficients: ', np.sum(clf_logistic.coef_<0))\n",
    "print('Number of null coefficients: ', np.sum(clf_logistic.coef_==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jH0ycYsdWSPg"
   },
   "source": [
    "The logistic regression performs pretty well, but all of the $7129$ features are used in the classification. It may be useful to select the most relevant ones by using regularized methods such as LASSO or Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOJxpAMUIx3f"
   },
   "source": [
    "## Classification using Elastic Net regularization\n",
    "\n",
    "The LASSO method we have seen in class uses a penalty function based on $ \\Vert\\beta\\Vert_1 = \\sum_{i=1} ^p |\\beta_i|$. The initial problem of the logistic regression becomes \n",
    "\n",
    "$$\\hat{\\beta} = \\underset{\\beta \\in \\mathbb{R}^p}{\\operatorname{argmax}} \\sum_{i=1}^n \\left[ y_i \\log p_i + (1- y_i) \\log (1 - p_i) \\right] - \\lambda \\Vert\\beta\\Vert_1 $$\n",
    "\n",
    "with $\\lambda >0$. However, one of the limitations of the LASSO method is that the solution has at most $\\min(n,p)$ non zero coefficients. For example, in the $p>>n$ case (our case), the LASSO selects at most $n$ variables before it saturates. In addition, if there is a group of highly correlated variables, then the LASSO tends to select one variable from a group and ignore the others. The Elastic Net method overcomes these problems by adding a quadratic part to the penalty. In reality, the Elastic Net is a regularized regression method that linearly combines the $l1$ and $l2$ penalties of the lasso and ridge methods. The problem becomes \n",
    "\n",
    "$$\\hat{\\beta} = \\underset{\\beta \\in \\mathbb{R}^p}{\\operatorname{argmax}} \\sum_{i=1}^n \\left[ y_i \\log p_i + (1- y_i) \\log (1 - p_i)\\right] - \\lambda_1 \\Vert\\beta\\Vert_1 - \\lambda_2 \\Vert\\beta\\Vert_2^2$$\n",
    "\n",
    "with $\\lambda_1,\\lambda_2 >0$. We can notice, when the $l2$ penalty is used alone this brings us back to the Ridge regression seen in class. This new solution can be sparse, but can also have as many non zero coefficients as it wants. In addition, the new problem is strictly convex and there is a grouping effect, i.e. if two features are very similar the elastic net is quite likely to select them both, unlike the LASSO. This new problem also requires to select the right coefficients $\\lambda_1$ and $\\lambda_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6KsbX9zqGQE"
   },
   "source": [
    "## Application of the Elastic Net method\n",
    "\n",
    "For the implementation, we will use the ```LogisticRegressionCV``` function from ```sklearn```, where we set ```penalty='elasticnet'```. In reality, to perform an Elastic Net regression with ```sklearn``` we have two hyper parameters to fine tune, the ```C```parameter and the ```l1_ratio```parameter. Let us consider the Elastic Net penalty function as implemented in the ```LogisticRegressionCV``` function:\n",
    "\n",
    "$$ \\mathcal{C}\\left(\\lambda \\Vert\\beta\\Vert_1 + (1-\\lambda)\\Vert\\beta\\Vert_2^2\\right)$$\n",
    "\n",
    "with $\\lambda \\in [0,1]$, and $\\mathcal{C} > 0$. The hyper parameter ```C``` is describing the inverse of the regularization strength, and the ```l1_ratio``` actually corresponds to the Elastic Net mixing parameter.\n",
    "\n",
    "#### The ```l1_ratio``` hyperparameter\n",
    "- A value of 0 is equivalent to using ```penalty='l2'```, which corresponds to the Ridge regression.\n",
    "- A value of 1 is equivalent to using ```penalty='l1'```, which corresponds to the LASSO regression.\n",
    "- A value of 0.5 is equivalent to having perfect balance between $l_1$ and $l_2$ penalties.\n",
    "\n",
    "#### The ```C``` hyperparameter\n",
    "- A small $\\mathcal{C}$ enforces a bigger restriction on the number of non zero coefficients, as it corresponds to a stronger regularization. In other terms, lowering $\\mathcal{C}$ strengthens the $\\lambda$ regulator, and thus shrinks the number of non zero coefficients.\n",
    "- A big $\\mathcal{C}$ relaxes the restriction on the number of non zero coefficients. \n",
    "\n",
    "Let us first try with the values ```C=1``` and ```l1_ratio=0.5```. This setup corresponds to a balanced ratio between $l_1$ and $l_2$ penalties, and a neutral regularization strength.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zldIPpBAC3wA",
    "outputId": "fc28d9b3-477b-4278-e115-21c40abb130b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 100.00%\n",
      "Accuracy on test set: 90.91%\n",
      "Number of positive coefficients:  112\n",
      "Number of negative coefficients:  81\n",
      "Number of null coefficients:  6936\n"
     ]
    }
   ],
   "source": [
    "clf_elasticnet = LogisticRegression(penalty = 'elasticnet', solver = 'saga', C = 1, max_iter=10000, l1_ratio= 0.5)\n",
    "clf_elasticnet.fit(X_train, y_train)\n",
    "print('Accuracy on train set: {:.2f}%'.format(clf_elasticnet.score(X_train, y_train)*100))\n",
    "print('Accuracy on test set: {:.2f}%'.format(clf_elasticnet.score(X_test, y_test)*100))\n",
    "print('Number of positive coefficients: ', np.sum(clf_elasticnet.coef_>0))\n",
    "print('Number of negative coefficients: ', np.sum(clf_elasticnet.coef_<0))\n",
    "print('Number of null coefficients: ', np.sum(clf_elasticnet.coef_==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDA2ApJ4oqyX"
   },
   "source": [
    "With the values for the hyper parameters that we chose, we can already see the Elastic Net yields a higher performance than the regular logistic regression  by scoring $90$% accuracy, and selects way less parameters. Indeed, the regularization allows us to have just a little more than $200$ non zero coefficients out of over $7000$, while the regular regression used all of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "droD5OwKAq-V"
   },
   "source": [
    "## Fine tuning of the hyper parameters using $k$-fold Cross Validation\n",
    "\n",
    "We are now interested in the impact of the hyper parameters' tuning on these results. A solution to choose these hyperparameters properly would be to perform a grid search using $k$-fold cross-validation. In this process, we define a range of values to be tested for ```C``` and ```l1_ratio```, we partition the training data in $k$ equals parts, and each of the parts is then set aside at turn as validation set while classification is performed on the other $k-1$ subsets using each one of the possible values for ```l1_ratio```and ```C```. The values selected are the ones for which the accuracy of the task is maximized. \n",
    "\n",
    "We try $10$ different values for ```C``` between $\\log(-10)$ and $\\log(10)$, $10$ values for ```l1_ratio``` in $[0,1]$, and select the values maximizing the accuracy score using $5$-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6-hJ0aNJWqh",
    "outputId": "62c5bd89-425a-4bdf-97e3-78855ccaa404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected regularization strength:  0.07742636826811278\n",
      "Selected L1 ratio :  0.1111111111111111\n",
      "Accuracy on train set: 100.00%\n",
      "Accuracy on test set: 95.45%\n",
      "Number of positive coefficients:  150\n",
      "Number of negative coefficients:  114\n",
      "Number of null coefficients:  6865\n"
     ]
    }
   ],
   "source": [
    "clf_cv = LogisticRegressionCV(cv = 5, penalty = 'elasticnet', solver = 'saga', Cs = np.logspace(-10,10,10), max_iter=10000, l1_ratios=np.linspace(0,1,10) )\n",
    "clf_cv.fit(X_train, y_train)\n",
    "print(\"Selected regularization strength: \", clf_cv.C_[0])\n",
    "print(\"Selected L1 ratio : \", clf_cv.l1_ratio_[0])\n",
    "print('Accuracy on train set: {:.2f}%'.format(clf_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on test set: {:.2f}%'.format(clf_cv.score(X_test, y_test)*100))\n",
    "print('Number of positive coefficients: ', np.sum(clf_cv.coef_>0))\n",
    "print('Number of negative coefficients: ', np.sum(clf_cv.coef_<0))\n",
    "print('Number of null coefficients: ', np.sum(clf_cv.coef_==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRczZQSrqJ_A"
   },
   "source": [
    "With the right choice of hyper parameters we reach an even higher accuracy of $95$%, although we notice this time we have a higher number of non null coefficients. There are two elements to take into account to explain this:\n",
    "- The selected ```C``` is equal to $0.077$. We know a small value for this hyper parameter means a stronger regularization, and therefore in theory we should have less non null coefficients than we had previously, as $0.077 < 1$. \n",
    "- However, the selected ```l1_ratio```  is equal to $0.11$. The reason is actually related to this ratio. Indeed, in the previous experiment, we chose ```l1_ratio=0.5``` which corresponded to having the same amount of $l1$ and $l2$ penalty. Here,  having ```l1_ratio=0.11``` corresponds to having a very strong $l2$ penalty, bringing the model closer to the Ridge regression. It makes perfect sense to have more non null coefficients with an important $l2$ penalty as the whole purpose of adding it was to overcome the problem we had with only the $l1$ penalty, by allowing the model to have as many non zero coefficients as it wants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goAzf9qTX61G"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We performed a binary classification on the high dimensional ```leukemia``` dataset using an Elastic Net regularized logistic regression. We compared the performance of the Elastic Net with a regular logistic regression and observed a significative improvement in performance as the accuracy on the task increased by almost $10$%. Even more importantly, the Elastic Net allows to reduce the number of features used in the classification by selecting only the relevant ones, and in our application we went from $7129$ initial features to approximately $260$. The main advantage of the Elastic Net over the LASSO is that it overcomes the restriction on the maximum number of non null coefficients the model can select, allowing the model to perform even better without this constraint. \n",
    "\n",
    "This second aspect is particularly relevant in the medical domain where $p>>n$ datasets are common, and where it can be extremely useful to perform classification using the less information possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
